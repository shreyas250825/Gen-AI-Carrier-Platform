#!/usr/bin/env python3
"""
Gemini Integration Verification

This script verifies that the Gemini API integration is working correctly
by testing the configuration, availability, and fallback system.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
backend_dir = Path(__file__).parent
env_file = backend_dir / ".env"
if env_file.exists():
    load_dotenv(env_file)

# Add backend to path
sys.path.append(str(backend_dir))

from app.ai_engines.engine_router import ai_engine_router

def main():
    """Verify Gemini integration status"""
    print("üîç Gemini Integration Verification")
    print("=" * 50)
    
    # Check environment variables
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    gemini_model = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
    
    print(f"üìã Configuration Check:")
    print(f"   GEMINI_API_KEY: {'‚úÖ Set' if gemini_api_key else '‚ùå Missing'}")
    print(f"   GEMINI_MODEL: {gemini_model}")
    print(f"   PREFER_OLLAMA: {os.getenv('PREFER_OLLAMA', 'true')}")
    print(f"   FALLBACK_TO_GEMINI: {os.getenv('FALLBACK_TO_GEMINI', 'true')}")
    
    # Check engine health
    print(f"\nüè• Engine Health Check:")
    health = ai_engine_router.health_check()
    
    ollama_status = "‚úÖ Available" if health['ollama']['available'] else "‚ùå Unavailable"
    gemini_status = "‚úÖ Available" if health['gemini']['available'] else "‚ùå Unavailable"
    
    print(f"   Ollama: {ollama_status}")
    if health['ollama']['available']:
        print(f"     Model: {health['ollama']['model']}")
        print(f"     URL: {health['ollama']['base_url']}")
    
    print(f"   Gemini: {gemini_status}")
    if health['gemini']['available']:
        print(f"     API Key: Configured")
        print(f"     Model: {gemini_model}")
    
    # Check router configuration
    print(f"\nüîÄ Router Configuration:")
    router_config = health['router']
    print(f"   Prefer Ollama: {'‚úÖ Yes' if router_config['prefer_ollama'] else '‚ùå No'}")
    print(f"   Fallback Enabled: {'‚úÖ Yes' if router_config['fallback_enabled'] else '‚ùå No'}")
    
    # Get usage statistics
    print(f"\nüìä Usage Statistics:")
    stats = ai_engine_router.get_engine_stats()
    print(f"   Ollama Requests: {stats['ollama_requests']}")
    print(f"   Gemini Requests: {stats['gemini_requests']}")
    print(f"   Fallback Events: {stats['fallback_count']}")
    print(f"   Last Engine Used: {stats['last_engine_used'] or 'None'}")
    
    # Test engine switching capability
    print(f"\nüîß Testing Engine Switching:")
    
    # Try to switch to Gemini
    if health['gemini']['available']:
        switch_success = ai_engine_router.force_engine("gemini")
        if switch_success:
            print("   ‚úÖ Can switch to Gemini")
            # Switch back to Ollama
            ai_engine_router.force_engine("ollama")
            print("   ‚úÖ Can switch back to Ollama")
        else:
            print("   ‚ùå Cannot switch to Gemini")
    else:
        print("   ‚ö†Ô∏è Gemini not available for switching")
    
    # Overall assessment
    print(f"\nüéØ Integration Status:")
    
    if health['gemini']['available'] and health['ollama']['available']:
        print("   ‚úÖ FULLY OPERATIONAL")
        print("   ‚Ä¢ Ollama available for local processing")
        print("   ‚Ä¢ Gemini available for cloud fallback")
        print("   ‚Ä¢ Automatic fallback system ready")
        print("   ‚Ä¢ Manual engine switching functional")
        
        print(f"\nüí° System Behavior:")
        print("   1. Primary: Uses Ollama for privacy and speed")
        print("   2. Fallback: Switches to Gemini if Ollama fails")
        print("   3. Manual: Can force specific engine via API")
        print("   4. Monitoring: Tracks usage and health stats")
        
        return True
        
    elif health['gemini']['available']:
        print("   ‚ö†Ô∏è GEMINI ONLY MODE")
        print("   ‚Ä¢ Gemini available for cloud processing")
        print("   ‚Ä¢ Ollama not available (local processing disabled)")
        print("   ‚Ä¢ System will use Gemini for all operations")
        
        return True
        
    elif health['ollama']['available']:
        print("   ‚ö†Ô∏è OLLAMA ONLY MODE")
        print("   ‚Ä¢ Ollama available for local processing")
        print("   ‚Ä¢ Gemini not available (no cloud fallback)")
        print("   ‚Ä¢ System will use Ollama for all operations")
        
        return False
        
    else:
        print("   ‚ùå NO ENGINES AVAILABLE")
        print("   ‚Ä¢ Neither Ollama nor Gemini is available")
        print("   ‚Ä¢ System cannot perform AI operations")
        
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nüéâ Gemini integration is working correctly!")
        print("The system is ready for production use with intelligent AI engine routing.")
    else:
        print(f"\n‚ö†Ô∏è Gemini integration needs attention.")
        print("Please check the configuration and ensure the API key is valid.")